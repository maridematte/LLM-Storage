---
title: "Representations of Binomials"
author: "Zachary Houghton"
date: "2024-04-28"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(reticulate)
myenvs = conda_list()
envname = myenvs$name[2]
use_condaenv(envname, required = T)
```

## Loading Data

```{r}
binomial_data = read_csv('../Data/corpus_sentences.csv')


```

## Semantic Representations

### Llama2 13b

```{python}

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, logging
from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig
import numpy as np

model_name_or_path = "TheBloke/Llama-2-13b-GPTQ"
model_basename = "model"

use_triton = False

tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)
tokenizer.pad_token = tokenizer.eos_token


model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,
        model_basename=model_basename,
        use_safetensors=True,
        trust_remote_code=True,
        device="cuda:0",
        use_triton=use_triton,
        quantize_config=None)
        
model.config.pad_token_id = model.config.eos_token_id
```

Batch vs single run

```{python}
from sklearn.metrics.pairwise import cosine_similarity

word1 = 'bread'
word2 = 'butter'
input_texts = 'Jimmy likes bread and butter'

input_ids = tokenizer(input_texts, padding = True, return_tensors = 'pt').input_ids
input_ids = input_ids.to(device)

start_index = input_ids[0].tolist().index(tokenizer.encode(word1)[1])
end_index = input_ids[0].tolist().index(tokenizer.encode(word2)[-1])

with torch.no_grad():
  outputs = model(input_ids, output_hidden_states = True)
  hidden_states = outputs[2]

token_vecs = hidden_states[-2][0]

  
phrase_embedding = token_vecs[start_index:end_index+1, :]

phrase_embedding = torch.mean(phrase_embedding, dim = 0)

##If batch running:

word1 = ['bread', 'radios']
word2 = ['butter', 'televisions']
input_texts = ['Jimmy likes bread and butter']


# Tokenize the batch
input_ids = tokenizer(input_texts, padding=True, return_tensors='pt').input_ids
input_ids = input_ids.to(device)


# Get the hidden states from the model
with torch.no_grad():
  outputs = model(input_ids, output_hidden_states=True)
  hidden_states = outputs[2]

second_to_last_layer = hidden_states[-2] #second to last hidden state

phrase_embeddings_batch_list = []

# Iterate over each input text in the batch
for i, (input_text, w1, w2) in enumerate(zip(input_texts, word1, word2)):
    
    

    # Find the indices of the words in the input text
    start_index = input_ids[i].tolist().index(tokenizer.encode(w1)[1])
    end_index = input_ids[i].tolist().index(tokenizer.encode(w2)[-1])
    print(start_index)
    print(end_index)
    

    # Select the batch
    token_vecs = second_to_last_layer[i]

    # Extract the phrase embedding
    phrase_embedding_batch = token_vecs[start_index:end_index + 1, :]

    # Compute the mean of the phrase embedding along the tokens dimension
    phrase_embedding_batch = torch.mean(phrase_embedding_batch, dim=0)

    # Append the phrase embedding to the list
    phrase_embeddings_batch_list.append(phrase_embedding_batch)

# Convert the list of tensors to a single tensor
phrase_embeddings_batch_list = torch.stack(phrase_embeddings_batch_list)

phrase_embeddings_batch_list.size()

# Print the semantic representations for the batch
print(phrase_embeddings_batch_list)


###Compare batch to individual runs

phrase_embeddings_batch_list = np.array(phrase_embeddings_batch_list.cpu())
phrase_embedding = np.array(phrase_embedding.cpu())
cosine_similarities = cosine_similarity([phrase_embeddings_batch_list[0]], [phrase_embedding])
print(cosine_similarities)
#0.99999 similarity, so they're doing the same thing
```

Let's make them functions:

```{python}
import pandas as pd

device = 'cuda:0' if torch.cuda.is_available() else 'cpu'

input_text = 'Jimmy likes bread and butter'
input_texts = ['Jimmy likes bread and butter']


def get_semantic_representation(model, tokenizer, input_text, word1, word2):
  
    word1 = word1
    word2 = word2
    input_text = input_text
    
    input_ids = tokenizer(input_text, padding = True, return_tensors = 'pt').input_ids
    input_ids = input_ids.to(device)
    
    start_index = input_ids[0].tolist().index(tokenizer.encode(word1)[1])
    end_index = input_ids[0].tolist().index(tokenizer.encode(word2)[-1])
    
    with torch.no_grad():
      outputs = model(input_ids, output_hidden_states = True)
      hidden_states = outputs[2]
    
    token_vecs = hidden_states[-2][0]
    
      
    phrase_embedding = token_vecs[start_index:end_index+1, :]
    
    phrase_embedding = torch.mean(phrase_embedding, dim = 0)
    
    return phrase_embedding
    
def get_semantic_representation_batch(model, tokenizer, input_texts, word1, word2):
  
    word1 = word1
    word2 = word2
    input_texts = input_texts
    
    input_ids = tokenizer(input_texts, padding = True, return_tensors = 'pt').input_ids
    input_ids = input_ids.to(device)
    
    # Get the hidden states from the model
    with torch.no_grad():
      outputs = model(input_ids, output_hidden_states=True)
      hidden_states = outputs[2]
    
    second_to_last_layer = hidden_states[-2] #second to last hidden state
    
    phrase_embeddings_batch_list = []
    
    # Iterate over each input text in the batch
    for i, (input_text, w1, w2) in enumerate(zip(input_texts, word1, word2)):
    
        # Find the indices of the words in the input text
        start_index = input_ids[i].tolist().index(tokenizer.encode(w1)[1])
        end_index = input_ids[i].tolist().index(tokenizer.encode(w2)[-1])
        #print(start_index)
        #print(end_index)
        
    
        # Select the batch
        token_vecs = second_to_last_layer[i]
    
        # Extract the phrase embedding
        phrase_embedding_batch = token_vecs[start_index:end_index + 1, :]
    
        # Compute the mean of the phrase embedding along the tokens dimension
        phrase_embedding_batch = torch.mean(phrase_embedding_batch, dim=0)
    
        # Append the phrase embedding to the list
        phrase_embeddings_batch_list.append(phrase_embedding_batch)
    
    # Convert the list of tensors to a single tensor
    phrase_embeddings_batch_list = torch.stack(phrase_embeddings_batch_list)
    
    return phrase_embeddings_batch_list
    
#test1 = get_semantic_representation(model, tokenizer, input_text, 'bread', 'butter')  
#test2 = get_semantic_representation_batch(model, tokenizer, input_texts, ['bread'], ['butter']) 

#phrase_embeddings_batch_list = np.array(test2.cpu())
#phrase_embedding = np.array(test1.cpu())
#cosine_similarities = cosine_similarity([phrase_embeddings_batch_list[0]], [phrase_embedding])
#print(cosine_similarities)
#cosine similarity of 0.99999 so the functions are doing the same thing 

test = [test[0].cpu().numpy(), test[1].cpu().numpy(), test[2].cpu().numpy()]

cosine_similarities = cosine_similarity(test)

print(cosine_similarities)

# Labels for each pair of tensors
labels = ['bread and butter', 'bread and butter (idiomatic)', 'butter and bread']

# Create a DataFrame with cosine similarities and labels
df = pd.DataFrame(cosine_similarities, index=labels, columns=labels)

# Print the labeled table
print(df)


def get_semantic_embedding_single_word(model, tokenizer, word):
    
    input_ids = tokenizer(word, padding = True, return_tensors = 'pt').input_ids
    input_ids = input_ids.to(device)
    
    # Get the hidden states from the model
    with torch.no_grad():
      outputs = model(input_ids, output_hidden_states=True)
      hidden_states = outputs[2]
    
    second_to_last_layer = hidden_states[-2] #second to last hidden state
    
    word_embeddings_batch_list = []
    
    # Iterate over each input text in the batch
    for i, word in enumerate(word):
      
        start_index = input_ids[i].tolist().index(tokenizer.encode(word)[1])
        end_index = input_ids[i].tolist().index(tokenizer.encode(word)[-1])
    
        token_vecs = second_to_last_layer[i]
        
        word_embedding_batch = token_vecs[start_index:end_index + 1, :]
    
        # Compute the mean of the phrase embedding along the tokens dimension
        word_embedding_batch = torch.mean(word_embedding_batch, dim=0)
    
    
        # Append the phrase embedding to the list
        word_embeddings_batch_list.append(word_embedding_batch)
    
    # Convert the list of tensors to a single tensor
    word_embeddings_batch_list = torch.stack(word_embeddings_batch_list)
    
    return word_embeddings_batch_list
  
  
def get_semantic_embedding_single_word_in_context(model, tokenizer, context, word):
    
    word = word
    input_text = context
    
    input_ids = tokenizer(input_text, padding = True, return_tensors = 'pt').input_ids
    input_ids = input_ids.to(device)
    
    # Get the hidden states from the model
    with torch.no_grad():
      outputs = model(input_ids, output_hidden_states=True)
      hidden_states = outputs[2]
    
    second_to_last_layer = hidden_states[-2] #second to last hidden state
    
    phrase_embeddings_batch_list = []
    
    # Iterate over each input text in the batch
    for i, (input_text, word) in enumerate(zip(input_text, word)):
    
        # Find the indices of the words in the input text
        start_index = input_ids[i].tolist().index(tokenizer.encode(word)[1])
        end_index = input_ids[i].tolist().index(tokenizer.encode(word)[-1])
        #print(start_index)
        #print(end_index)
        
    
        # Select the batch
        token_vecs = second_to_last_layer[i]
    
        # Extract the phrase embedding
        phrase_embedding_batch = token_vecs[start_index:end_index + 1, :]
    
        # Compute the mean of the phrase embedding along the tokens dimension
        phrase_embedding_batch = torch.mean(phrase_embedding_batch, dim=0)
    
        # Append the phrase embedding to the list
        phrase_embeddings_batch_list.append(phrase_embedding_batch)
    
    # Convert the list of tensors to a single tensor
    phrase_embeddings_batch_list = torch.stack(phrase_embeddings_batch_list)
    
    return phrase_embeddings_batch_list
  
sentences = ['bank robber', 'river bank', 'vault', 'shore', 'bank']

words = ['bank', 'bank', 'vault', 'shore', 'bank']

test1 = get_semantic_embedding_single_word(model, tokenizer, ['bank'])
test2 = get_semantic_embedding_single_word_in_context(model, tokenizer, sentences, words)


test = [test2[0].cpu().numpy(), test2[1].cpu().numpy(), test2[2].cpu().numpy(), test2[3].cpu().numpy(), test2[4].cpu().numpy(), test1[0].cpu().numpy()]

cosine_similarities = cosine_similarity(test)

print(cosine_similarities)

labels = ['bank (robber)', 'bank (river)', 'vault', 'shore', 'bank (without context)', 'bank (without context single word function)']

# Create a DataFrame with cosine similarities and labels
df = pd.DataFrame(cosine_similarities, index=labels, columns=labels)

print(df)
```

## Visualization

```{python}
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from scipy.spatial import distance

input_texts = ['I like to eat bread and butter', "Running a competition smoothly is his bread and butter", "Jimmy's favorite foods are butter and bread"]

word1 = ['bread', 'bread', 'butter']
word2 = ['butter', 'butter', 'bread']

test = get_semantic_representation_batch(model, tokenizer, input_texts, word1, word2)


test_np = test.cpu().numpy()




pca = PCA(n_components=2)
reduced_features = pca.fit_transform(test_np)

#labels
labels = ['bread and butter', 'bread and butter (idiomatic)', 'butter and bread']

# Plot PCA with labels and arrows
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.scatter(reduced_features[:, 0], reduced_features[:, 1])
for i, label in enumerate(labels):
    plt.text(reduced_features[i, 0], reduced_features[i, 1], label, fontsize=10, ha='right')

# Plot arrows connecting each pair of points with labels denoting the distance
for i in range(len(reduced_features)):
    for j in range(i + 1, len(reduced_features)):
        plt.arrow(reduced_features[i, 0], reduced_features[i, 1],
                  reduced_features[j, 0] - reduced_features[i, 0],
                  reduced_features[j, 1] - reduced_features[i, 1],
                  color='gray', alpha=0.5, head_width=0.05, width=0.005)

        # Calculate the Euclidean distance between the points
        dist = distance.euclidean(reduced_features[i], reduced_features[j])
        # Annotate the arrow with the distance
        plt.text((reduced_features[i, 0] + reduced_features[j, 0]) / 2,
                 (reduced_features[i, 1] + reduced_features[j, 1]) / 2,
                 f'{dist:.2f}', fontsize=8, ha='center', va='center')

plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('2D Visualization of All Features')

plt.show()
```
